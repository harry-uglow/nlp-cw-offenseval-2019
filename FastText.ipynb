{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "def clean_str(string, test=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    words = string.replace('\\\"','').split(' ')\n",
    "    for idx, word in enumerate(words):\n",
    "        if word == '@USER' or word == 'URL':\n",
    "            continue\n",
    "        elif test and len(word) > 0 and word[0] == '@':\n",
    "            words[idx] = '@USER'\n",
    "            continue\n",
    "\n",
    "        word = re.sub(r'^https?:\\/\\/.*', 'URL', word)\n",
    "        word = re.sub(r\"[^A-Za-z0-9()@,!?\\'\\`]\", \" \", word)\n",
    "        word = re.sub(r\"\\'s\", \" \\'s\", word)\n",
    "        word = re.sub(r\"\\'ve\", \" have\", word)\n",
    "        word = re.sub(r\"n\\'t\", \" not\", word)\n",
    "        word = re.sub(r\"\\'re\", \" are\", word)\n",
    "        word = re.sub(r\"\\'d\", \" \\'d\", word)\n",
    "        word = re.sub(r\"\\'ll\", \" will\", word)\n",
    "        word = re.sub(r\",\", \" , \", word)\n",
    "        word = re.sub(r\"!\", \" ! \", word)\n",
    "        word = re.sub(r\"\\(\", \" \\( \", word)\n",
    "        word = re.sub(r\"\\)\", \" \\) \", word)\n",
    "        word = re.sub(r\"\\?\", \" \\? \", word)\n",
    "        word = re.sub(r\"\\s{2,}\", \" \", word)\n",
    "        words[idx] = word.strip().lower()\n",
    "    return ' '.join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_fast_format(input_name, output_name, header, tweet_index=1, task_index=2):\n",
    "    labels = []\n",
    "    with open(input_name) as csv_file:\n",
    "        with open(output_name, 'w', newline='') as output_file:\n",
    "            label_name = '__label__'\n",
    "            csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "\n",
    "            line_count = 0\n",
    "            rows = []\n",
    "            for row in csv_reader:\n",
    "                if line_count == 0 and header:\n",
    "                    print(f'Column names are {\", \".join(row)}')\n",
    "                    line_count += 1\n",
    "                else:\n",
    "                    tweet = tweet_cleaner(row[tweet_index].replace('\"',''))\n",
    "                    label = row[task_index].replace('\"','')\n",
    "                    if label != 'NULL':\n",
    "                        labels.append(label)\n",
    "                        to_save = f'{label_name}{label} {tweet}'\n",
    "                        rows.append([to_save])\n",
    "\n",
    "\n",
    "            fast_writer = csv.writer(output_file, delimiter=',')\n",
    "            fast_writer.writerows(rows)\n",
    "            return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "def find_best_fast_params(labels, train_name, predicted_name, test_name, first_label='NOT', second_label='OFF', third_label=None, fourth_label=None, average='macro',):\n",
    "    ground_truth = pd.DataFrame(np.array(labels), columns=['label'])\n",
    "    mapping = {first_label: 0, second_label: 1}\n",
    "    if third_label:\n",
    "        mapping[third_label] = 2\n",
    "    if fourth_label:\n",
    "        mapping[fourth_label] = 3\n",
    "    ground_truth = ground_truth.label.map(mapping)\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    param_grid = [{'epoch': [5, 10, 20, 30, 50], 'wordNgrams': [1,2,3,4,5], 'lr': [0.01, 0.05, 0.1, 0.5, 1], 'minCount': range(2, 10, 2),  'ws':  range(2, 10, 2), 'dim': [50,100]} ]\n",
    "    params_list = ParameterGrid(param_grid)\n",
    "    selected = np.random.choice(params_list, size = 300, replace=False)\n",
    "    \n",
    "    for param in tqdm(selected):\n",
    "        epoch = param['epoch']\n",
    "        wordNgrams = param['wordNgrams']\n",
    "        lr = param['lr']\n",
    "        minCount = param['minCount']\n",
    "        ws = param['ws']\n",
    "        dim = param['dim']\n",
    "        os.system(f'./fastNext/fasttext supervised -input {train_name} -output not_trained -lr {lr} -ws {ws} -minCount {minCount} -epoch {epoch} -wordNgrams {wordNgrams} -dim {dim}')\n",
    "        os.system(f'./fastNext/fasttext predict not_trained.bin {test_name} > {predicted_name}')\n",
    "        predicted = pd.read_csv(predicted_name, header=None, names=['label'])\n",
    "        predicted['label'] = predicted.label.str.replace('__label__','')\n",
    "        predicted = predicted.label.map(mapping)\n",
    "\n",
    "        score = f1_score(predicted, ground_truth, average=average)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = param\n",
    "            copyfile('not_trained.bin', f'best_trained-{first_label}-{second_label}.bin')\n",
    "            \n",
    "    return (best_score, best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are id, tweet, subtask_a, subtask_b, subtask_c\n"
     ]
    }
   ],
   "source": [
    "convert_file_to_fast_format('./data/start-kit/training-v1/offenseval-training-v1.tsv', './data/train-fast-a.txt', True,1,2)\n",
    "labels_1 = convert_file_to_fast_format('./data/start-kit/trial-data/offenseval-trial.txt', './data/test-fast-a.txt', False, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are id, tweet, subtask_a, subtask_b, subtask_c\n"
     ]
    }
   ],
   "source": [
    "convert_file_to_fast_format('./data/start-kit/training-v1/offenseval-training-v1.tsv', './data/train-fast-b.txt', True,1,3)\n",
    "labels_2 = convert_file_to_fast_format('./data/start-kit/trial-data/offenseval-trial.txt', './data/test-fast-b.txt', False, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are id, tweet, subtask_a, subtask_b, subtask_c\n"
     ]
    }
   ],
   "source": [
    "convert_file_to_fast_format('./data/start-kit/training-v1/offenseval-training-v1.tsv', './data/train-fast-c.txt', True,1,4)\n",
    "labels_3 = convert_file_to_fast_format('./data/start-kit/trial-data/offenseval-trial.txt', './data/test-fast-c.txt', False, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/300 [00:23<23:27,  4.77s/it]/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "100%|██████████| 300/300 [16:38<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "(best_score_a, best_params_a) = find_best_fast_params(labels_1, train_name='./data/train-fast-a.txt',test_name='./data/test-fast-a.txt', predicted_name='./data/predicted-fast-a.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7849462365591398,\n",
       " {'dim': 100, 'epoch': 20, 'lr': 0.1, 'minCount': 4, 'wordNgrams': 3, 'ws': 4})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(best_score_a, best_params_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [14:32<00:00,  2.88s/it]\n"
     ]
    }
   ],
   "source": [
    "(best_score_b, best_params_b) = find_best_fast_params(labels_2, train_name='./data/train-fast-b.txt',test_name='./data/test-fast-b.txt', predicted_name='./data/predicted-fast-b.txt',first_label='UNT', second_label='TIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6450354609929078,\n",
       " {'dim': 50, 'epoch': 10, 'lr': 1, 'minCount': 4, 'wordNgrams': 1, 'ws': 8})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(best_score_b, best_params_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [14:07<00:00,  2.83s/it]\n"
     ]
    }
   ],
   "source": [
    "(best_score_c, best_params_c) = find_best_fast_params(labels_3, train_name='./data/train-fast-c.txt',test_name='./data/test-fast-c.txt', predicted_name='./data/predicted-fast-c.txt',first_label='IND', second_label='OTH', third_label='GRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42745098039215684,\n",
       " {'dim': 50, 'epoch': 20, 'lr': 0.01, 'minCount': 2, 'wordNgrams': 1, 'ws': 2})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(best_score_c, best_params_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_fast_format(input_name, output_name):\n",
    "    labels = []\n",
    "    with open(input_name) as csv_file:\n",
    "        with open(output_name, 'w', newline='') as output_file:\n",
    "            label_name = '__label__'\n",
    "            csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "            rows = []\n",
    "            for row in csv_reader:\n",
    "                    tweet = tweet_cleaner(row[1].replace('\"',''))\n",
    "                    rows.append([tweet])\n",
    "\n",
    "\n",
    "            fast_writer = csv.writer(output_file, delimiter=',')\n",
    "            fast_writer.writerows(rows)\n",
    "return convert_file_to_fast_format('./data/C/test_set_taskc.tsv', './formatted-c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ids(labels, ids):\n",
    "    with open(labels, 'r') as textfile1, open(ids, 'r') as textfile2:\n",
    "        rows = []\n",
    "        no = 0 \n",
    "#         print(textfile1.read())\n",
    "        for x, y in zip(textfile1.readlines(), textfile2.readlines()):\n",
    "            if no != 0:\n",
    "                x = x.replace('__label__', '').strip()\n",
    "                y = y.split('\\t')[0].strip()\n",
    "#                 combined = ', '.join([x,y]).strip('\\n')\n",
    "                rows.append([y, x])\n",
    "            no = no + 1\n",
    "    with open(labels, 'w') as output_file:\n",
    "        fast_writer = csv.writer(output_file, delimiter=',')\n",
    "        fast_writer.writerows(rows)\n",
    "#         return rows\n",
    "rows = add_ids('./tested-c.txt', './data/C/test_set_taskc.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15923 NOT'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
